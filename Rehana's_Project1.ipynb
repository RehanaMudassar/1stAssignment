{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RehanaMudassar/1stAssignment/blob/main/Rehana's_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-google-genai"
      ],
      "metadata": {
        "id": "Hih-39BJCCyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8510f5-cc48-46cd-c864-b943d4ccdd84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.8.3)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (0.3.25)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-google-genai) (2.10.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.155.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.25.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.25.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (9.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.2.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.2.2)\n",
            "Downloading langchain_google_genai-2.0.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-google-genai\n",
            "Successfully installed filetype-1.2.0 langchain-google-genai-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "3WCwD3LSCFD7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community"
      ],
      "metadata": {
        "id": "lgDGj0ZZCFG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55cfa33e-3500-46b7-cf37-6ffa7c6d2c37"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.25 (from langchain)\n",
            "  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain_community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.13 langchain-core-0.3.28 langchain_community-0.3.13 marshmallow-3.23.2 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "0HaQkESZCFLk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.7,\n",
        "    max_tokens=None,\n",
        "    timeout=None\n",
        "   )\n",
        "llm.invoke(\"Hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj-zCoegHxaa",
        "outputId": "14c35094-a3fb-42c8-a87d-5a51202170c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I help you today?\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-ad61abc2-ec49-42ab-9887-589e80e7d07e-0', usage_metadata={'input_tokens': 2, 'output_tokens': 10, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# Create the LLM chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Run the chain with a sample question\n",
        "question = \"What is LangChain?\"\n",
        "response = chain.run({\"question\": question})\n",
        "\n",
        "print(\"Answer:\", response)"
      ],
      "metadata": {
        "id": "wL_dcoq9CNLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8468c26e-fece-4aa5-8663-a791e2d7f5f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-e64963ffdf64>:7: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangChain is a framework designed to simplify the development of applications using large language models (LLMs).  It provides a standard interface and abstractions for common LLM tasks, making it easier to manage and use LLMs in various applications.  Think of it as a toolbox filled with components specifically designed for interacting with and enhancing the capabilities of LLMs.\n",
            "\n",
            "Here's a breakdown of its key features and benefits:\n",
            "\n",
            "* **Components:** LangChain offers modular components like prompts, LLMs, agents, memory, indexes, and chains. These components can be combined and customized to create complex workflows.\n",
            "* **Use Cases:** It supports a wide range of applications, including chatbots, generative question-answering, summarization, and code generation.\n",
            "* **Simplified Interaction:** LangChain streamlines the interaction with LLMs by handling prompt management, result parsing, and other common tasks.  This reduces boilerplate code and allows developers to focus on the application logic.\n",
            "* **Chain Management:**  \"Chains\" are sequences of calls to LLMs or other utilities. LangChain provides tools to manage these chains, making it easier to create complex workflows and control the flow of information.\n",
            "* **Agent Orchestration:**  LangChain allows the use of \"agents\" that can decide which tools to use based on the user's input, enabling more dynamic and interactive applications.\n",
            "* **Memory Management:** It provides mechanisms to maintain context and history across conversations, allowing LLMs to remember previous interactions and provide more coherent responses.\n",
            "* **Data Connection:**  Facilitates connecting LLMs to external data sources, enabling them to access and process information from various sources.\n",
            "\n",
            "In essence, LangChain helps developers build LLM-powered applications more efficiently by providing a structured and flexible framework. It reduces the complexity of working with LLMs and allows developers to focus on creating innovative and useful applications.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKapXBW-CNOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation_chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "\n",
        "user_inputs = [\n",
        "    \"Hello, can you explain what LangChain is?\",\n",
        "    \"How does memory work in LangChain?\",\n",
        "    \"Can LangChain be used with Google Gemini for applications?\"\n",
        "]\n",
        "\n",
        "for user_input in user_inputs:\n",
        "    response = conversation_chain.run(user_input)\n",
        "    print(f\"User: {user_input}\")\n",
        "    print(f\"Assistant: {response}\\n\")\n",
        "\n",
        "\n",
        "print(\"Conversation Memory:\")\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "id": "l3mTnb8HCZg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf35c2d1-a09f-4f5b-fc10-b853e79c5876"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hello, can you explain what LangChain is?\n",
            "Assistant: Hello!  LangChain is a framework designed to simplify the development of applications using large language models (LLMs).  Think of it as a toolbox filled with components you can assemble to build powerful and complex LLM-driven systems.  It's not a model itself, but rather a way to chain together different components and connect LLMs to other sources of data.\n",
            "\n",
            "Some key features and components within LangChain include:\n",
            "\n",
            "* **LLM Wrappers:**  These provide a standardized interface for interacting with various LLMs like OpenAI's GPT models, Cohere's models, and Hugging Face Hub models. This allows developers to easily switch between different providers without rewriting large portions of their code.  It also handles things like API calls and rate limiting.\n",
            "\n",
            "* **Prompt Templates:** These help structure the input to the LLM, making it easier to create consistent and effective prompts.  They can include placeholders for variables, allowing for dynamic prompt generation. For example, you could have a template for summarizing text that includes a placeholder for the text to be summarized.\n",
            "\n",
            "* **Chains:** This is the core concept of LangChain.  Chains allow you to sequence multiple components together.  For instance, you could create a chain that first uses an LLM to generate questions about a given piece of text, then uses another LLM to answer those questions based on the same text.\n",
            "\n",
            "* **Indexes:**  These structure your data in a way that makes it easier for LLMs to access and process it.  Different index types are suited for different types of data and use cases.  For example, a vectorstore index can be used for similarity search within a large document collection.\n",
            "\n",
            "* **Agents:** Agents allow LLMs to interact with their environment.  They can decide what actions to take (like searching the web or accessing a database) based on the current context and then use the results to inform further actions.  LangChain provides tools for creating agents and integrating them with different tools.\n",
            "\n",
            "* **Memory:**  This allows chains and agents to retain information across multiple interactions.  This is crucial for building conversational applications where the LLM needs to remember previous turns in the conversation.  Various memory implementations are available, from simple buffers to more complex vector databases.\n",
            "\n",
            "\n",
            "By combining these components, LangChain enables developers to build a wide range of applications, including chatbots, question-answering systems, summarization tools, and generative applications. It simplifies many of the common challenges associated with working with LLMs, making it easier to build robust and scalable LLM-powered applications.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: How does memory work in LangChain?\n",
            "Assistant: LangChain provides several different ways to manage memory, allowing you to persist state between calls to a chain or agent.  This is essential for building conversational applications where the LLM needs to remember previous interactions.  Here's a breakdown of the core memory concepts:\n",
            "\n",
            "* **ConversationBufferMemory:** This is the simplest form of memory.  It simply stores all previous messages in a buffer.  You can access the entire conversation history or just the last `k` turns. It's easy to use but can become inefficient for long conversations.\n",
            "\n",
            "* **ConversationBufferWindowMemory:** Similar to `ConversationBufferMemory`, but only keeps the last `k` interactions in memory, discarding older ones. This helps manage the size of the memory and improves efficiency for extended conversations.\n",
            "\n",
            "* **ChatMessageHistory:** This is a more structured way to store conversation history. It stores messages as instances of `ChatMessage`, which include additional metadata like the speaker's role (e.g., \"human\", \"ai\").  This allows for more sophisticated handling of conversation history and can be used with other memory types.\n",
            "\n",
            "* **ConversationSummaryMemory:**  This memory type summarizes the conversation history, keeping the memory compact.  It uses an LLM to generate the summary, which can then be included in the prompt for subsequent calls.  This approach is useful for managing long conversations, but the summarization process can introduce information loss.\n",
            "\n",
            "* **ConversationSummaryBufferMemory:** This combines the summarization capabilities of `ConversationSummaryMemory` with the buffering mechanism of `ConversationBufferMemory`. It stores both the full conversation history and a running summary.\n",
            "\n",
            "* **Entity Memory:** This type of memory focuses on tracking specific entities mentioned throughout the conversation.  It's useful for remembering details about things like people, places, or objects.\n",
            "\n",
            "* **VectorStore-Backed Memory:** This is a more advanced approach that uses a vector database to store and retrieve memories.  It allows for more complex querying and retrieval based on semantic similarity.  This is particularly useful for building knowledge-intensive applications.  You would typically use embeddings created from the conversation history and store them in a vectorstore like FAISS or Pinecone.  Then, you can retrieve relevant memories based on the current input.\n",
            "\n",
            "\n",
            "Choosing the right memory type depends on the specific needs of your application. For simple chatbots, `ConversationBufferMemory` or `ConversationBufferWindowMemory` might be sufficient.  For more complex applications requiring long-term memory or knowledge retrieval, a `VectorStore-Backed Memory` or `Entity Memory` might be more appropriate.  You can even chain different memory types together for more sophisticated memory management.\n",
            "\n",
            "\n",
            "User: Can LangChain be used with Google Gemini for applications?\n",
            "Assistant: Yes, LangChain can be used with Google Gemini.  LangChain maintains integrations with a variety of Large Language Models (LLMs), including those provided by Google.  While specific implementation details might evolve as both LangChain and Gemini develop, the general approach involves using the appropriate LLM wrapper within LangChain.\n",
            "\n",
            "You would typically use the `GoogleGemini` class from the `langchain.llms` module. This class allows you to interact with Gemini models much like you would with other LLMs supported by LangChain.  You can provide your Google Cloud project information and select the specific Gemini model you want to use.\n",
            "\n",
            "Then, you can integrate this `GoogleGemini` instance into other LangChain components like chains, agents, and memory modules, allowing you to build complete applications powered by Gemini.  For example, you could build a chatbot that uses Gemini for natural language understanding and generation, combined with a vector database for knowledge retrieval and a conversation memory module to maintain context across interactions.\n",
            "\n",
            "\n",
            "Conversation Memory:\n",
            "Human: Hello, can you explain what LangChain is?\n",
            "AI: Hello!  LangChain is a framework designed to simplify the development of applications using large language models (LLMs).  Think of it as a toolbox filled with components you can assemble to build powerful and complex LLM-driven systems.  It's not a model itself, but rather a way to chain together different components and connect LLMs to other sources of data.\n",
            "\n",
            "Some key features and components within LangChain include:\n",
            "\n",
            "* **LLM Wrappers:**  These provide a standardized interface for interacting with various LLMs like OpenAI's GPT models, Cohere's models, and Hugging Face Hub models. This allows developers to easily switch between different providers without rewriting large portions of their code.  It also handles things like API calls and rate limiting.\n",
            "\n",
            "* **Prompt Templates:** These help structure the input to the LLM, making it easier to create consistent and effective prompts.  They can include placeholders for variables, allowing for dynamic prompt generation. For example, you could have a template for summarizing text that includes a placeholder for the text to be summarized.\n",
            "\n",
            "* **Chains:** This is the core concept of LangChain.  Chains allow you to sequence multiple components together.  For instance, you could create a chain that first uses an LLM to generate questions about a given piece of text, then uses another LLM to answer those questions based on the same text.\n",
            "\n",
            "* **Indexes:**  These structure your data in a way that makes it easier for LLMs to access and process it.  Different index types are suited for different types of data and use cases.  For example, a vectorstore index can be used for similarity search within a large document collection.\n",
            "\n",
            "* **Agents:** Agents allow LLMs to interact with their environment.  They can decide what actions to take (like searching the web or accessing a database) based on the current context and then use the results to inform further actions.  LangChain provides tools for creating agents and integrating them with different tools.\n",
            "\n",
            "* **Memory:**  This allows chains and agents to retain information across multiple interactions.  This is crucial for building conversational applications where the LLM needs to remember previous turns in the conversation.  Various memory implementations are available, from simple buffers to more complex vector databases.\n",
            "\n",
            "\n",
            "By combining these components, LangChain enables developers to build a wide range of applications, including chatbots, question-answering systems, summarization tools, and generative applications. It simplifies many of the common challenges associated with working with LLMs, making it easier to build robust and scalable LLM-powered applications.\n",
            "\n",
            "Human: How does memory work in LangChain?\n",
            "AI: LangChain provides several different ways to manage memory, allowing you to persist state between calls to a chain or agent.  This is essential for building conversational applications where the LLM needs to remember previous interactions.  Here's a breakdown of the core memory concepts:\n",
            "\n",
            "* **ConversationBufferMemory:** This is the simplest form of memory.  It simply stores all previous messages in a buffer.  You can access the entire conversation history or just the last `k` turns. It's easy to use but can become inefficient for long conversations.\n",
            "\n",
            "* **ConversationBufferWindowMemory:** Similar to `ConversationBufferMemory`, but only keeps the last `k` interactions in memory, discarding older ones. This helps manage the size of the memory and improves efficiency for extended conversations.\n",
            "\n",
            "* **ChatMessageHistory:** This is a more structured way to store conversation history. It stores messages as instances of `ChatMessage`, which include additional metadata like the speaker's role (e.g., \"human\", \"ai\").  This allows for more sophisticated handling of conversation history and can be used with other memory types.\n",
            "\n",
            "* **ConversationSummaryMemory:**  This memory type summarizes the conversation history, keeping the memory compact.  It uses an LLM to generate the summary, which can then be included in the prompt for subsequent calls.  This approach is useful for managing long conversations, but the summarization process can introduce information loss.\n",
            "\n",
            "* **ConversationSummaryBufferMemory:** This combines the summarization capabilities of `ConversationSummaryMemory` with the buffering mechanism of `ConversationBufferMemory`. It stores both the full conversation history and a running summary.\n",
            "\n",
            "* **Entity Memory:** This type of memory focuses on tracking specific entities mentioned throughout the conversation.  It's useful for remembering details about things like people, places, or objects.\n",
            "\n",
            "* **VectorStore-Backed Memory:** This is a more advanced approach that uses a vector database to store and retrieve memories.  It allows for more complex querying and retrieval based on semantic similarity.  This is particularly useful for building knowledge-intensive applications.  You would typically use embeddings created from the conversation history and store them in a vectorstore like FAISS or Pinecone.  Then, you can retrieve relevant memories based on the current input.\n",
            "\n",
            "\n",
            "Choosing the right memory type depends on the specific needs of your application. For simple chatbots, `ConversationBufferMemory` or `ConversationBufferWindowMemory` might be sufficient.  For more complex applications requiring long-term memory or knowledge retrieval, a `VectorStore-Backed Memory` or `Entity Memory` might be more appropriate.  You can even chain different memory types together for more sophisticated memory management.\n",
            "\n",
            "Human: Can LangChain be used with Google Gemini for applications?\n",
            "AI: Yes, LangChain can be used with Google Gemini.  LangChain maintains integrations with a variety of Large Language Models (LLMs), including those provided by Google.  While specific implementation details might evolve as both LangChain and Gemini develop, the general approach involves using the appropriate LLM wrapper within LangChain.\n",
            "\n",
            "You would typically use the `GoogleGemini` class from the `langchain.llms` module. This class allows you to interact with Gemini models much like you would with other LLMs supported by LangChain.  You can provide your Google Cloud project information and select the specific Gemini model you want to use.\n",
            "\n",
            "Then, you can integrate this `GoogleGemini` instance into other LangChain components like chains, agents, and memory modules, allowing you to build complete applications powered by Gemini.  For example, you could build a chatbot that uses Gemini for natural language understanding and generation, combined with a vector database for knowledge retrieval and a conversation memory module to maintain context across interactions.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}